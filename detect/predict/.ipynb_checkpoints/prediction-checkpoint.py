import os
import argparse
import pandas as pd
import numpy as np
from tqdm import tqdm
from transformers import pipeline
from transformers import T5Tokenizer, T5ForConditionalGeneration, AutoConfig, AutoModelForCausalLM, AutoTokenizer
#from utils.model_utils import adjust_prediction_score
import torch

def get_opts():
    arg = argparse.ArgumentParser()
    arg.add_argument(
        "--your-team-name",
        type=str,
        default="nailong_test",
        help="wenailong"
    )
    arg.add_argument(
        "--data-path",
        type=str,
        help="Path to the CSV dataset",
        default="/home/sankuai/data/llm_safety/detect/eval/UCAS_AISAD_TEXT-test1.csv"
    )
    arg.add_argument(
        "--model-path",
        type=str,
        help="Path of model to use",
        default="/home/sankuai/data/LLaMA-Factory/saves/qwen3-01/full/sft"
        #/home/sankuai/data/huggingface.co/Qwen/Qwen3-8B
        #/home/sankuai/data/LLaMA-Factory/saves/qwen3/lora/merge-checkpoint
    )
    arg.add_argument(
        "--result-path",
        type=str,
        help="Path to save the results",
        default="/home/sankuai/data/llm_safety/detect/saves/result/"
    )
    opts = arg.parse_args()
    return opts

def get_dataset(opts):
    print(f"Loading dataset from {opts.data_path}...")
    data = pd.read_csv(opts.data_path)

    # New format: prompt, text
    dataset = data[['prompt', 'text']].dropna().copy()
    print(f"Prepared dataset with {len(dataset)} prompts")
    
    return dataset

def get_model(opts):
    print(f"Loading {opts.model_path} detector model...")
    
    # if opts.model_type == "argugpt":
    #     model = pipeline("text-classification", model="SJTU-CL/RoBERTa-large-ArguGPT-sent", 
    #                      max_length=512, truncation=True)
    '''
    You should load your model here!
    '''
    device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
    tokenizer = AutoTokenizer.from_pretrained(opts.model_path, cache_dir=None)
    model = AutoModelForCausalLM.from_pretrained(opts.model_path,
                                                device_map='auto',
                                                torch_dtype=torch.float16 if device == 'cuda'
                                                else torch.float32,
                                                cache_dir=None).eval()

    
    print("Model loaded successfully")
    return tokenizer, model

def run_prediction(tokenizer, model, dataset):
    print("Starting prediction process...")
    prompts = dataset['prompt'].tolist()
    texts = dataset['text'].tolist()
    #prompts = prompts[1000:1100]
    #texts = texts[1000:1100]
    text_predictions = []
    device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
    start_time = pd.Timestamp.now()
    templates = "Below is a query and its corresponding answer. Please determine if the answer was generated by a human or by a language model. Output 1 for the former and 0 for the latter.\nQuery:{query}\nAnswer:{answer}\nPlease output only '1' or '0', regardless of the language of the query and answer. DO NOT say any additional words or explain."
    #\nQuery:{query}\nAnswer:{answer}\nPlease output only '1' or '0', regardless of the language of the query and answer. DO NOT say any additional words or explain.
    # Process texts
    print("Processing texts...")
    for prompt, text in tqdm(zip(prompts, texts), total=len(texts), desc="Text predictions"):
        params = {'query': prompt, 'answer': text}
        input_text = templates.format(**params)
        token_0_id = tokenizer.convert_tokens_to_ids("0")
        token_1_id = tokenizer.convert_tokens_to_ids("1")
        #print(f"token_0_id {token_0_id}  token_1_id {token_1_id}")
        inputs = tokenizer(input_text, return_tensors="pt").to(device)
        input_ids = inputs.input_ids
        output = model.generate(
            input_ids=input_ids,
            max_new_tokens=2,  # 假设生成多个 token 用于分析
            output_scores=True,
            return_dict_in_generate=True,
        )
        #print(tokenizer.decode(785))
        generated_tokens = output.sequences[:, input_ids.shape[-1]:]
        # 获取每个位置的概率分布
        scores = output.scores  # 这是一个tuple，每个元素对应一个位置的分数
        #print(scores[3][0][15],scores[3][0][16])
        #preds = [output.outputs[0].text for result in results]
        #token_ids = [output.outputs[0].token_ids for result in results]
        #print(preds,token_ids)
        # 将分数转换为概率
        probs = []
        for score in scores:
            # 对每个位置的分数应用 softmax
            #print(score[0][15],score[0][16])
            prob = torch.softmax(torch.stack([score[0][15],score[0][16]], dim=0), dim=0)
            #print(prob)
            #prob = torch.softmax(score,dim=-1)
            probs.append(prob)
        
        # 获取每个位置生成的token的概率
        token_probs = []
        for i, token_id in enumerate(generated_tokens[0]):
            # 获取该位置上实际生成的token的概率
            #prob = probs[i][0][token_id]
            #print(probs[i])
            if token_id.item() == 15:
                prob_1 = probs[i][1]
                # 1.0-prob
                break
            if token_id.item() == 16:
                #prob
                prob_1 = probs[i][1]
                break
            #token_probs.append(prob.item())
            #print(token_id,prob.item())
        
        # 解码生成的token
        #generated_text = tokenizer.decode(generated_tokens[0])
        #print(generated_text)
        #if generated_tokens[0][0] == 15 or generated_tokens[0][0] == 16:
        #prob_1 = token_probs[1]
        #print(prob_1)
        #input("输入k继续")
        '''
        logits = output.scores[4][0]
        probs = torch.softmax(logits, dim=-1)

        # 提取 "0" 和 "1" 的概率
        prob_0 = probs[token_0_id].item()
        prob_1 = probs[token_1_id].item()

    
        # 获取前 N 个最可能的 token 及其概率
        N = 5  # 你可以调整这个数字来获取更多或更少的 token
        top_probs, top_indices = torch.topk(probs, N)
    
        # 解码 token 并打印概率
        for prob, index in zip(top_probs, top_indices):
            token = tokenizer.decode([index])
            print(f"Token: {token}, Probability: {prob.item():.4f}")
            # Adjust score - higher values indicate more likely human-written text
        '''
        final_score = prob_1.item()
            
        text_predictions.append(final_score)
    end_time = pd.Timestamp.now()
    processing_time = (end_time - start_time).total_seconds()
    
    # Create results in the requested format
    results_data = {
        'prompt': prompts,
        'text_prediction': text_predictions
    }
    
    # Create results dictionary
    results = {
        "predictions_data": results_data,
        "time": processing_time
    }
    
    print(f"Predictions completed in {processing_time:.2f} seconds")
    return results

if __name__ == "__main__":
    opts = get_opts()
    dataset = get_dataset(opts)
    tokenizer, model = get_model(opts)
    results = run_prediction(tokenizer, model, dataset)
    
    # Save results
    os.makedirs(opts.result_path, exist_ok=True)
    writer = pd.ExcelWriter(os.path.join(opts.result_path, opts.your_team_name + ".xlsx"), engine='openpyxl')
    
    # Create prediction dataframe with the required columns
    prediction_frame = pd.DataFrame(
        data = results["predictions_data"]
    )
    
    # Filter out rows with None values
    prediction_frame = prediction_frame.dropna()
    
    time_frame = pd.DataFrame(
        data = {
            "Data Volume": [len(prediction_frame)],
            "Time": [results["time"]],
        }
    )
    
    prediction_frame.to_excel(writer, sheet_name="predictions", index=False)
    time_frame.to_excel(writer, sheet_name="time", index=False)
    writer.close()
    
    print(f"Results saved to {os.path.join(opts.result_path, opts.your_team_name + '.xlsx')}")
