import os
import argparse
import pandas as pd
import numpy as np
from tqdm import tqdm
from transformers import pipeline
from transformers import T5Tokenizer, T5ForConditionalGeneration, AutoConfig, AutoModelForCausalLM, AutoTokenizer
#from utils.model_utils import adjust_prediction_score
import torch

def get_opts():
    arg = argparse.ArgumentParser()
    arg.add_argument(
        "--your-team-name",
        type=str,
        default="nailong_test",
        help="队伍名称"
    )
    arg.add_argument(
        "--data-path",
        type=str,
        help="Path to the CSV dataset",
        default="/home/sankuai/data/llm_safety/detect/eval/UCAS_AISAD_TEXT-test1.csv"
    )
    arg.add_argument(
        "--model-path",
        type=str,
        help="Path of model to use",
        default="/home/sankuai/data/LLaMA-Factory/saves/qwen3-01/full/sft"
        #/home/sankuai/data/huggingface.co/Qwen/Qwen3-8B
        #/home/sankuai/data/LLaMA-Factory/saves/qwen3/lora/merge-checkpoint
    )
    arg.add_argument(
        "--result-path",
        type=str,
        help="Path to save the results",
        default="/home/sankuai/data/llm_safety/detect/saves/result/"
    )
    opts = arg.parse_args()
    return opts

def get_dataset(opts):
    print(f"Loading dataset from {opts.data_path}...")
    data = pd.read_csv(opts.data_path)

    # New format: prompt, text
    dataset = data[['prompt', 'text']].dropna().copy()
    print(f"Prepared dataset with {len(dataset)} prompts")
    
    return dataset

def get_model(opts):
    print(f"Loading {opts.model_path} detector model...")
    
    # if opts.model_type == "argugpt":
    #     model = pipeline("text-classification", model="SJTU-CL/RoBERTa-large-ArguGPT-sent", 
    #                      max_length=512, truncation=True)
    '''
    You should load your model here!
    '''
    device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
    tokenizer = AutoTokenizer.from_pretrained(opts.model_path, cache_dir=None)
    model = AutoModelForCausalLM.from_pretrained(opts.model_path,
                                                device_map='auto',
                                                torch_dtype=torch.float16 if device == 'cuda'
                                                else torch.float32,
                                                cache_dir=None).eval()

    
    print("Model loaded successfully")
    return tokenizer, model

def run_prediction(tokenizer, model, dataset):
    print("Starting prediction process...")
    prompts = dataset['prompt'].tolist()
    texts = dataset['text'].tolist()
    #prompts = prompts[1000:1100]
    #texts = texts[1000:1100]
    text_predictions = []
    device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
    start_time = pd.Timestamp.now()
    templates = "Below is a query and its corresponding answer. Please determine if the answer was generated by a human or by a language model. Output 1 for the former and 0 for the latter.\nQuery:{query}\nAnswer:{answer}\nPlease output only '1' or '0', regardless of the language of the query and answer. DO NOT say any additional words or explain."
    print("Processing texts...")
    for prompt, text in tqdm(zip(prompts, texts), total=len(texts), desc="Text predictions"):
        params = {'query': prompt, 'answer': text}
        input_text = templates.format(**params)
        token_0_id = tokenizer.convert_tokens_to_ids("0")
        token_1_id = tokenizer.convert_tokens_to_ids("1")
        inputs = tokenizer(input_text, return_tensors="pt").to(device)
        input_ids = inputs.input_ids
        output = model.generate(
            input_ids=input_ids,
            max_new_tokens=2,  # 假设生成多个 token 用于分析
            output_scores=True,
            return_dict_in_generate=True,
        )
        generated_tokens = output.sequences[:, input_ids.shape[-1]:]
        # 获取每个位置的概率分布
        scores = output.scores  # 这是一个tuple，每个元素对应一个位置的分数
        # 将分数转换为概率，只考虑0和1
        probs = []
        for score in scores:
            prob = torch.softmax(torch.stack([score[0][15],score[0][16]], dim=0), dim=0)
            probs.append(prob)
        
        # 获取最终生成答案的概率
        token_probs = []
        for i, token_id in enumerate(generated_tokens[0]):
            # 获取该位置上实际生成的token的概率
            if token_id.item() == 15:
                prob_1 = probs[i][1]
                # 1.0-prob
                break
            if token_id.item() == 16:
                #prob
                prob_1 = probs[i][1]
                break
        
        final_score = prob_1.item()  
        text_predictions.append(final_score)
    end_time = pd.Timestamp.now()
    processing_time = (end_time - start_time).total_seconds()
    
    # Create results in the requested format
    results_data = {
        'prompt': prompts,
        'text_prediction': text_predictions
    }
    
    # Create results dictionary
    results = {
        "predictions_data": results_data,
        "time": processing_time
    }
    
    print(f"Predictions completed in {processing_time:.2f} seconds")
    return results

if __name__ == "__main__":
    opts = get_opts()
    dataset = get_dataset(opts)
    tokenizer, model = get_model(opts)
    results = run_prediction(tokenizer, model, dataset)
    
    # Save results
    os.makedirs(opts.result_path, exist_ok=True)
    writer = pd.ExcelWriter(os.path.join(opts.result_path, opts.your_team_name + ".xlsx"), engine='openpyxl')
    
    # Create prediction dataframe with the required columns
    prediction_frame = pd.DataFrame(
        data = results["predictions_data"]
    )
    
    # Filter out rows with None values
    prediction_frame = prediction_frame.dropna()
    
    time_frame = pd.DataFrame(
        data = {
            "Data Volume": [len(prediction_frame)],
            "Time": [results["time"]],
        }
    )
    
    prediction_frame.to_excel(writer, sheet_name="predictions", index=False)
    time_frame.to_excel(writer, sheet_name="time", index=False)
    writer.close()
    
    print(f"Results saved to {os.path.join(opts.result_path, opts.your_team_name + '.xlsx')}")
